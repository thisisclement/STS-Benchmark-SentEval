{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import evaluation, SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"stsb-xlm-r-multilingual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_labse = SentenceTransformer(\"LaBSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8122a7a279df4476b1d748ca1744d2b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/1.01G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_para_xlm_r = SentenceTransformer(\"paraphrase-xlm-r-multilingual-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d9ba267164b4742ad386c49dab000ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/83.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 1.2.0, however, your version is 1.1.1. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_para_minilm_l6 = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1379"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data\n",
    "data_path = \"/Users/clementow/repos/translate_selenium/data/sts-benchmark/sts-test-translate.csv\"\n",
    "\n",
    "with open(data_path) as fopen:\n",
    "    dataset = list(filter(None, fopen.read().split('\\n')))\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"One woman is measuring another woman's ankle.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Seorang wanita mengukur buku lali wanita lain.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'5.000'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(dataset[2].split('\\t')[5])\n",
    "display(dataset[2].split('\\t')[6])\n",
    "dataset[2].split('\\t')[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = []\n",
    "sent2 = []\n",
    "scores = []\n",
    "for data in dataset:\n",
    "    data_list = data.split('\\t')\n",
    "    sent1.append(data_list[5])\n",
    "    sent2.append(data_list[6])\n",
    "    scores.append(data_list[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1379"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1379"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1379"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(len(sent1), len(sent2), len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_stsbenchmark_dataset(data_path=\"/Users/clementow/repos/translate_selenium/data/sts-benchmark/sts-test-translate.csv\"):\n",
    "\n",
    "    with open(data_path) as fopen:\n",
    "        dataset = list(filter(None, fopen.read().split('\\n')))\n",
    "    \n",
    "    sent1 = []\n",
    "    sent2 = []\n",
    "    scores = []\n",
    "    for data in dataset:\n",
    "        data_list = data.split('\\t')\n",
    "        sent1.append(data_list[5])\n",
    "        sent2.append(data_list[6])\n",
    "        scores.append(data_list[4])\n",
    "    return sent1, sent2, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load eng original dataset\n",
    "# read data\n",
    "data_path = \"/Users/clementow/repos/translate_selenium/data/sts-benchmark/sts-test.csv\"\n",
    "\n",
    "sent1_en, sent2_en, scores_en = prep_stsbenchmark_dataset(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1379, 1379, 1379)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent1_en), len(sent2_en), len(scores_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6895, 1379, 1379)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent1), len(sent2), len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1379"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1379"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = len(sent1)\n",
    "\n",
    "sent1.extend(sent2)\n",
    "display(idx)\n",
    "len(sent1[:idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Seorang gadis memberus rambutnya.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1[idx:][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By right we can use this but this is more used when training and deals with the number of epochs and steps\n",
    "```python\n",
    "sts_evaluator = evaluation.EmbeddingSimilarityEvaluator(sentences1=sent1, sentences2=sent2, scores=scores)\n",
    "\n",
    "# cannot use this as it is called only during training. \n",
    "# what i want is the just direct evaluation after the embeddings computation stage\n",
    "sts_evaluator.__call__(model) \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics.pairwise import paired_cosine_distances, paired_euclidean_distances, paired_manhattan_distances\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List\n",
    "from enum import Enum\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "\n",
    "\n",
    "class SimilarityFunction(Enum):\n",
    "    COSINE = 0\n",
    "    EUCLIDEAN = 1\n",
    "    MANHATTAN = 2\n",
    "    DOT_PRODUCT = 3\n",
    "\n",
    "class EmbeddingSimilarityEval:\n",
    "    def __init__(self, model: SentenceTransformer, sentences1: List[str], sentences2: List[str], scores: List[float], batch_size: int = 16, main_similarity: SimilarityFunction = None, name: str = '', show_progress_bar: bool = False, write_csv: bool = True):\n",
    "        \"\"\"\n",
    "        Constructs an evaluator based for the dataset\n",
    "\n",
    "        The labels need to indicate the similarity between the sentences.\n",
    "        \n",
    "        :param models: Model that you want to evaluate with\n",
    "        :param sentences1:  List with the first sentence in a pair\n",
    "        :param sentences2: List with the second sentence in a pair\n",
    "        :param scores: Similarity score between sentences1[i] and sentences2[i]\n",
    "        :param write_csv: Write results to a CSV file\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.sentences1 = sentences1\n",
    "        self.sentences2 = sentences2\n",
    "        self.scores = [float(i) for i in scores]\n",
    "        self.write_csv = write_csv\n",
    "        \n",
    "        assert model != None and type(model) is SentenceTransformer\n",
    "        assert len(self.sentences1) == len(self.sentences2)\n",
    "        assert len(self.sentences1) == len(self.scores)\n",
    "\n",
    "        self.main_similarity = main_similarity\n",
    "        self.name = name\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        if show_progress_bar is None:\n",
    "            show_progress_bar = (logger.getEffectiveLevel() == logging.INFO or logger.getEffectiveLevel() == logging.DEBUG)\n",
    "        self.show_progress_bar = show_progress_bar\n",
    "\n",
    "        self.csv_file = \"similarity_evaluation\"+(\"_\"+name if name else '')+\"_results.csv\"\n",
    "        self.csv_headers = [\"cosine_pearson\", \"cosine_spearman\", \"euclidean_pearson\", \"euclidean_spearman\", \"manhattan_pearson\", \"manhattan_spearman\", \"dot_pearson\", \"dot_spearman\"]\n",
    "        \n",
    "        \n",
    "    def encode_embeddings(self):\n",
    "        all_sent = list()\n",
    "        #note down the sent1 end index\n",
    "        sent1_end_idx = len(self.sentences1)\n",
    "        #join both sent1 and sent2 into the same list\n",
    "        all_sent.extend(self.sentences1)\n",
    "        all_sent.extend(self.sentences2)\n",
    "        self.sentences = all_sent\n",
    "        embeddings = self.model.encode(self.sentences, convert_to_numpy=True, show_progress_bar=self.show_progress_bar)\n",
    "        return embeddings[:sent1_end_idx], embeddings[sent1_end_idx:]\n",
    "    \n",
    "   \n",
    "\n",
    "    def run_eval(self, output_path: str = None):\n",
    "        embeddings1, embeddings2 = self.encode_embeddings()\n",
    "        labels = self.scores\n",
    "        \n",
    "        cosine_scores = 1 - (paired_cosine_distances(embeddings1, embeddings2))\n",
    "        manhattan_distances = -paired_manhattan_distances(embeddings1, embeddings2)\n",
    "        euclidean_distances = -paired_euclidean_distances(embeddings1, embeddings2)\n",
    "        dot_products = [np.dot(emb1, emb2) for emb1, emb2 in zip(embeddings1, embeddings2)]\n",
    "        \n",
    "        eval_pearson_cosine, _ = pearsonr(labels, cosine_scores)\n",
    "        eval_spearman_cosine, _ = spearmanr(labels, cosine_scores)\n",
    "        \n",
    "#         eval_pearson_manhattan, _ = pearsonr(labels, manhattan_distances)\n",
    "#         eval_spearman_manhattan, _ = spearmanr(labels, manhattan_distances)\n",
    "\n",
    "#         eval_pearson_euclidean, _ = pearsonr(labels, euclidean_distances)\n",
    "#         eval_spearman_euclidean, _ = spearmanr(labels, euclidean_distances)\n",
    "\n",
    "#         eval_pearson_dot, _ = pearsonr(labels, dot_products)\n",
    "#         eval_spearman_dot, _ = spearmanr(labels, dot_products)\n",
    "\n",
    "        if output_path is not None and self.write_csv:\n",
    "            csv_path = os.path.join(output_path, self.csv_file)\n",
    "            output_file_exists = os.path.isfile(csv_path)\n",
    "            with open(csv_path, mode=\"a\" if output_file_exists else 'w', encoding=\"utf-8\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                if not output_file_exists:\n",
    "                    writer.writerow(self.csv_headers)\n",
    "                    \n",
    "                writer.writerow([eval_pearson_cosine, eval_spearman_cosine, '',\n",
    "                                 '', '', '', '', ''])\n",
    "#                 writer.writerow([eval_pearson_cosine, eval_spearman_cosine, eval_pearson_euclidean,\n",
    "#                                  eval_spearman_euclidean, eval_pearson_manhattan, eval_spearman_manhattan, eval_pearson_dot, eval_spearman_dot])\n",
    "\n",
    "\n",
    "        if self.main_similarity == SimilarityFunction.COSINE:\n",
    "            print(\"Cosine-Similarity :\\tPearson: {:.4f}\\tSpearman: {:.4f}\".format(\n",
    "                eval_pearson_cosine, eval_spearman_cosine))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc431bb0f25b4e93aad45d23d7dda707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine-Similarity :\tPearson: 0.8361\tSpearman: 0.8412\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sts_eval = EmbeddingSimilarityEval(model_para_minilm_l6, sent1_en, sent2_en, scores, main_similarity=SimilarityFunction.COSINE, name=\"paraphrase-MiniLM-L6-v2\", show_progress_bar=True, write_csv=True)\n",
    "sts_eval.run_eval(output_path='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Sanity Check]\n",
    "The above tallies with the Spearman's Correlation score as in https://www.sbert.net/docs/pretrained_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f97848ebc775444db70a4b192ba7121e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c34f31f5e522478e83b1b02eecc7ad42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine-Similarity :\tPearson: 0.2100\tSpearman: 0.1696\n"
     ]
    }
   ],
   "source": [
    "sts_eval = EmbeddingSimilarityEval(model_para_minilm_l6, sent1, sent2, scores, main_similarity=SimilarityFunction.COSINE, name=\"paraphrase-MiniLM-L6-v2 with Malay\", show_progress_bar=True, write_csv=True)\n",
    "sts_eval.run_eval(output_path='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51a5874a8ffa4306ba8c6f99be9ce4ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca775d355364dcf8e5b7e81d75c73a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine-Similarity :\tPearson: 0.8379\tSpearman: 0.8504\n"
     ]
    }
   ],
   "source": [
    "sts_eval = EmbeddingSimilarityEval(model, sent1_en, sent2_en, scores, main_similarity=SimilarityFunction.COSINE, name=\"stsb-xlm-r-multilingual\", show_progress_bar=True, write_csv=True)\n",
    "sts_eval.run_eval(output_path='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29027e8c06ce44148ca5904bfae8507d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfbffd2aafe7429cb79e900a4854f2b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine-Similarity :\tPearson: 0.8379\tSpearman: 0.8504\n"
     ]
    }
   ],
   "source": [
    "sts_eval = EmbeddingSimilarityEval(model, sent1_en, sent2_en, scores, main_similarity=SimilarityFunction.COSINE, name=\"stsb-xlm-r-multilingual\", show_progress_bar=True)\n",
    "sts_eval.run_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66d14dc6cc904443baf5966beb410954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a68d8e73a54453aa9c4740443005c6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine-Similarity :\tPearson: 0.6067\tSpearman: 0.6337\n"
     ]
    }
   ],
   "source": [
    "sts_eval = EmbeddingSimilarityEval(model, sent1, sent2, scores, main_similarity=SimilarityFunction.COSINE, name=\"stsb-xlm-r-multilingual with Malay data\", show_progress_bar=True)\n",
    "sts_eval.run_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "939925fedfb940bd88a3bd1f83a18600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57378aa3837b40618b5545143c37bac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine-Similarity :\tPearson: 0.8355\tSpearman: 0.8350\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b9229a78014cdc8e856224ca799892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc21754d77e64ad298224e5f3cfbdc78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine-Similarity :\tPearson: 0.5571\tSpearman: 0.6027\n"
     ]
    }
   ],
   "source": [
    "sts_eval = EmbeddingSimilarityEval(model_para_xlm_r, sent1_en, sent2_en, scores, main_similarity=SimilarityFunction.COSINE, name=\"Paraphrase XML-R\", show_progress_bar=True)\n",
    "sts_eval.run_eval()\n",
    "\n",
    "sts_eval = EmbeddingSimilarityEval(model_para_xlm_r, sent1, sent2, scores, main_similarity=SimilarityFunction.COSINE, name=\"Paraphrase XML-R with Malay data\", show_progress_bar=True)\n",
    "sts_eval.run_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e96e653b8541968f7fb9f256c29497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb2e1f8d1a68467e85987ab687c9a865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine-Similarity :\tPearson: 0.7269\tSpearman: 0.7225\n"
     ]
    }
   ],
   "source": [
    "sts_eval = EmbeddingSimilarityEval(model_labse, sent1_en, sent2_en, scores, main_similarity=SimilarityFunction.COSINE, name=\"LaBSE\", show_progress_bar=True)\n",
    "sts_eval.run_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11885f5356804414a43d5126abab39a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd5f0f5a5cd64f8fa315092c06c95cc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine-Similarity :\tPearson: 0.4690\tSpearman: 0.5319\n"
     ]
    }
   ],
   "source": [
    "sts_eval = EmbeddingSimilarityEval(model_labse, sent1, sent2, scores, main_similarity=SimilarityFunction.COSINE, name=\"LaBSE with Malay data\", show_progress_bar=True)\n",
    "sts_eval.run_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sts_benchmark",
   "language": "python",
   "name": "sts_benchmark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
